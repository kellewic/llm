{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyM7cE2k6hU8mILDZEM0iqgC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Fine Tuning\n","Had to switch from my lab VM to Google Collab for this since I needed a GPU."],"metadata":{"id":"pfzdmQyac8bQ"}},{"cell_type":"code","source":["!pip install transformers datasets evaluate transformers[torch] py7zr peft wandb"],"metadata":{"collapsed":true,"id":"G_kbSuMddBxN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Full fine-tuning for summarization"],"metadata":{"id":"Duvn4E52d6_l"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from datasets import load_dataset\n","import wandb"],"metadata":{"id":"pz7FePq3GD5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load model and tokenizer\n","BASE_MODEL = \"facebook/bart-large-cnn\"\n","\n","base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n","base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)"],"metadata":{"collapsed":true,"id":"bm9o-4CDeKFZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Load dataset"],"metadata":{"id":"xyxAGhm8ey74"}},{"cell_type":"code","source":["dataset = load_dataset(\"knkarthick/samsum\")\n","\n","## Clean dataset\n","dataset = dataset.remove_columns(['id'])\n","dataset = dataset.filter(lambda example: example['dialogue'] is not None)\n","\n","## Shrink dataset for training\n","PERCENT = 0.3\n","\n","dataset['train'] = dataset['train'].shuffle(seed=42).select(range(int(len(dataset['train'])*PERCENT)))\n","dataset['test'] = dataset['test'].shuffle(seed=37).select(range(int(len(dataset['test'])*PERCENT)))\n","dataset['validation'] = dataset['validation'].shuffle(seed=4).select(range(int(len(dataset['validation'])*PERCENT)))\n","\n","dataset"],"metadata":{"collapsed":true,"id":"hsE0vNbNfP8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test summarization of base model"],"metadata":{"id":"UgPFuGr7hEZh"}},{"cell_type":"code","source":["SAMPLE_DATA = dataset['test'][0]\n","\n","def generate_summary(input, model, tokenizer, isPeft=False):\n","    sample = input['dialogue']\n","    label = input['summary']\n","\n","    prompt = f\"\"\"\n","    Summarize the following conversation.\n","\n","    {sample}\n","\n","    Summary:\n","    \"\"\"\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","    output = model.generate(input_ids=input_ids[\"input_ids\"], max_new_tokens=200)\n","    output = tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","    print(\"Sample\")\n","    print(sample)\n","    print(\"----------------------------------------\")\n","    print(\"Model Generated Summary\")\n","    print(output)\n","    print(\"Correct Summary\")\n","    print(label)"],"metadata":{"collapsed":true,"id":"wjwMAyk3fsTd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_summary(SAMPLE_DATA, base_model, base_tokenizer)"],"metadata":{"id":"08-J7QPU6KWj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Prepare the dataset"],"metadata":{"id":"71AYdMl4hRp1"}},{"cell_type":"code","source":["def tokenize_inputs(example):\n","    start_prompt = 'Summarize the following conversation.\\n\\n'\n","    end_prompt = '\\n\\nSummary: '\n","\n","    # Tokenize inputs\n","    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n","    model_inputs = tokenizer(prompt, padding=\"max_length\", max_length=200, truncation=True)\n","\n","    # Tokenize labels\n","    labels = tokenizer(example[\"summary\"], padding=\"max_length\", max_length=200, truncation=True)\n","\n","    labels[\"input_ids\"] = [\n","        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n","        for label_seq in labels[\"input_ids\"]\n","    ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenizer = base_tokenizer\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenized_dataset = dataset.map(tokenize_inputs, batched=True)\n","tokenized_dataset = tokenized_dataset.remove_columns(['dialogue', 'summary'])\n","tokenized_dataset"],"metadata":{"collapsed":true,"id":"S3S2oLLvhVh8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Start training"],"metadata":{"id":"WK487169nJx8"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()\n","\n","HF_USER = \"shayharding\"\n","FT_MODEL = \"bart-samsum-finetuned\""],"metadata":{"id":"iHSKwStanPSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./\" + FT_MODEL,\n","    hub_model_id=HF_USER + \"/\" + FT_MODEL,\n","    learning_rate=5e-6,\n","    num_train_epochs=1,\n","    weight_decay=0.01,\n","    auto_find_batch_size=True,\n","    #per_device_train_batch_size=4,\n","    #gradient_accumulation_steps=4,\n","    #warmup_steps=100,\n","    eval_strategy=\"epoch\",\n","    logging_steps=10\n",")\n","\n","trainer = Trainer(\n","    model=base_model,\n","    processing_class=base_tokenizer,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"]\n",")\n","\n","wandb.init(project=FT_MODEL)"],"metadata":{"id":"9LrIP7AlnYGL","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"collapsed":true,"id":"gBzXBIeloZHY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"id":"WttjMrItqaRM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test the full fine-tuned model"],"metadata":{"id":"HTsPf_lWqwJT"}},{"cell_type":"code","source":["ft_tokenizer = AutoTokenizer.from_pretrained(HF_USER + \"/\" + FT_MODEL)\n","ft_model = AutoModelForSeq2SeqLM.from_pretrained(HF_USER + \"/\" + FT_MODEL)\n","\n","generate_summary(SAMPLE_DATA, ft_model, ft_tokenizer)"],"metadata":{"id":"fOj-cG88qy0j","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create PEFT model using LoRA"],"metadata":{"id":"tJ-rnk7Zs7sm"}},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model, TaskType\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=TaskType.SEQ_2_SEQ_LM\n",")\n","\n","peft_model = get_peft_model(ft_model, lora_config)\n","\n","PEFT_MODEL = \"bart-samsum-peft\""],"metadata":{"id":"rZS0kXhYuMei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"./\" + PEFT_MODEL,\n","    hub_model_id=HF_USER + \"/\" + PEFT_MODEL,\n","    learning_rate=5e-6,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    auto_find_batch_size=True,\n","    #per_device_train_batch_size=4,\n","    #gradient_accumulation_steps=4,\n","    #warmup_steps=100,\n","    eval_strategy=\"epoch\",\n","    logging_steps=10\n",")\n","\n","trainer = Trainer(\n","    model=peft_model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"]\n",")\n","\n","wandb.init(project=PEFT_MODEL)"],"metadata":{"id":"GePryNtzvACF","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["peft_model.print_trainable_parameters()"],"metadata":{"id":"3iZGeoPavY6-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"collapsed":true,"id":"Ah96jWtSvce5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"id":"Dkw6sP0N3rfM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Test the PEFT model"],"metadata":{"id":"Qz5LVqeT4mCo"}},{"cell_type":"code","source":["from peft import PeftModel\n","\n","loaded_peft_model = PeftModel.from_pretrained(ft_model, HF_USER + \"/\" + PEFT_MODEL, is_trainable=False)"],"metadata":{"id":"1Sf4euDZ31S4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_summary(SAMPLE_DATA, loaded_peft_model, ft_tokenizer)"],"metadata":{"id":"zBJvFQ9T5URT"},"execution_count":null,"outputs":[]}]}