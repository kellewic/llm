{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfzdmQyac8bQ"
   },
   "source": [
    "## Fine Tuning\n",
    "Had to switch from my lab VM to Google Collab for this since I needed a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "G_kbSuMddBxN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate transformers[torch] py7zr peft wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Duvn4E52d6_l"
   },
   "source": [
    "### Full fine-tuning for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz7FePq3GD5-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bm9o-4CDeKFZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Load model and tokenizer\n",
    "BASE_MODEL = \"facebook/bart-large-cnn\"\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyxAGhm8ey74"
   },
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "hsE0vNbNfP8x",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"knkarthick/samsum\")\n",
    "\n",
    "## Clean dataset\n",
    "dataset = dataset.remove_columns(['id'])\n",
    "dataset = dataset.filter(lambda example: example['dialogue'] is not None)\n",
    "\n",
    "## Shrink dataset for training\n",
    "PERCENT = 0.3\n",
    "\n",
    "dataset['train'] = dataset['train'].shuffle(seed=42).select(range(int(len(dataset['train'])*PERCENT)))\n",
    "dataset['test'] = dataset['test'].shuffle(seed=37).select(range(int(len(dataset['test'])*PERCENT)))\n",
    "dataset['validation'] = dataset['validation'].shuffle(seed=4).select(range(int(len(dataset['validation'])*PERCENT)))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgPFuGr7hEZh"
   },
   "source": [
    "#### Test summarization of base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wjwMAyk3fsTd",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_DATA = dataset['test'][0]\n",
    "\n",
    "def generate_summary(input, model, tokenizer, isPeft=False):\n",
    "    sample = input['dialogue']\n",
    "    label = input['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversation.\n",
    "\n",
    "    {sample}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(input_ids=input_ids[\"input_ids\"], max_new_tokens=200)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Sample\")\n",
    "    print(sample)\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Model Generated Summary\")\n",
    "    print(output)\n",
    "    print(\"Correct Summary\")\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08-J7QPU6KWj"
   },
   "outputs": [],
   "source": [
    "generate_summary(SAMPLE_DATA, base_model, base_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71AYdMl4hRp1"
   },
   "source": [
    "#### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "S3S2oLLvhVh8",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_inputs(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "\n",
    "    # Tokenize inputs\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    model_inputs = tokenizer(prompt, padding=\"max_length\", max_length=200, truncation=True)\n",
    "\n",
    "    # Tokenize labels\n",
    "    labels = tokenizer(example[\"summary\"], padding=\"max_length\", max_length=200, truncation=True)\n",
    "\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n",
    "        for label_seq in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenizer = base_tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_dataset = dataset.map(tokenize_inputs, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['dialogue', 'summary'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK487169nJx8"
   },
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHSKwStanPSH"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "HF_USER = \"shayharding\"\n",
    "FT_MODEL = \"bart-samsum-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "9LrIP7AlnYGL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\" + FT_MODEL,\n",
    "    hub_model_id=HF_USER + \"/\" + FT_MODEL,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    auto_find_batch_size=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    processing_class=base_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "wandb.init(project=FT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "gBzXBIeloZHY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WttjMrItqaRM"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTsPf_lWqwJT"
   },
   "source": [
    "#### Test the full fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fOj-cG88qy0j",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ft_tokenizer = AutoTokenizer.from_pretrained(HF_USER + \"/\" + FT_MODEL)\n",
    "ft_model = AutoModelForSeq2SeqLM.from_pretrained(HF_USER + \"/\" + FT_MODEL)\n",
    "\n",
    "generate_summary(SAMPLE_DATA, ft_model, ft_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJ-rnk7Zs7sm"
   },
   "source": [
    "### Create PEFT model using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZS0kXhYuMei"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(ft_model, lora_config)\n",
    "\n",
    "PEFT_MODEL = \"bart-samsum-peft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "GePryNtzvACF",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\" + PEFT_MODEL,\n",
    "    hub_model_id=HF_USER + \"/\" + PEFT_MODEL,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    auto_find_batch_size=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"]\n",
    ")\n",
    "\n",
    "wandb.init(project=PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iZGeoPavY6-"
   },
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ah96jWtSvce5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dkw6sP0N3rfM"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz5LVqeT4mCo"
   },
   "source": [
    "#### Test the PEFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sf4euDZ31S4"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_peft_model = PeftModel.from_pretrained(ft_model, HF_USER + \"/\" + PEFT_MODEL, is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBJvFQ9T5URT"
   },
   "outputs": [],
   "source": [
    "generate_summary(SAMPLE_DATA, loaded_peft_model, ft_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM7cE2k6hU8mILDZEM0iqgC",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
