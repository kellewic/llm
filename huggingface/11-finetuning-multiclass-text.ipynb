{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPpyZYuRftVIrJSZOcElOd9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Fine-Tuning Multiclass Text Classification"],"metadata":{"id":"psQ-JIRlj8De"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjEqRwFGZ0bl","collapsed":true},"outputs":[],"source":["!pip install transformers datasets accelerate seaborn bertviz umap-learn wandb"]},{"cell_type":"markdown","source":["### Get and View Dataset"],"metadata":{"id":"dDuIWh6ikYKi"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"dair-ai/emotion\")\n","dataset"],"metadata":{"collapsed":true,"id":"qLCHg4DlkbwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Let's peek at the data\n","train = dataset[\"train\"]\n","df = train.to_pandas()\n","label_names = train.features['label'].names\n","\n","print(df.head())\n","print()\n","print(label_names)"],"metadata":{"collapsed":true,"id":"VGV976mhk2eq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Let's add the class labels into the dataframe\n","df['label_text'] = df['label'].apply(lambda x: label_names[x])\n","df.head()"],"metadata":{"collapsed":true,"id":"3WBEN-CzltJy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","label_counts = df['label_text'].value_counts(ascending=True)\n","\n","label_counts.plot(kind='barh')\n","plt.title(\"Class Distribution\")\n","plt.show()"],"metadata":{"collapsed":true,"id":"2O84Dsl4vMo2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prepare Dataset"],"metadata":{"id":"CavwwerowDFx"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","MODEL = \"google-bert/bert-base-uncased\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)"],"metadata":{"collapsed":true,"id":"sW0jIGsmwDZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from datasets import Dataset, DatasetDict\n","\n","train_df, test_df = train_test_split(df, test_size=0.3, stratify=df['label_text'])\n","test_df, val_df = train_test_split(test_df, test_size=1/3, stratify=test_df['label'])\n","\n","print(f\"Train shape: {train_df.shape}\")\n","print(f\"Test shape: {test_df.shape}\")\n","print(f\"Validation shape: {val_df.shape}\")\n","print()\n","\n","dataset = DatasetDict({\n","    'train': Dataset.from_pandas(train_df, preserve_index=False),\n","    'test': Dataset.from_pandas(test_df, preserve_index=False),\n","    'validation': Dataset.from_pandas(val_df, preserve_index=False)\n","})\n","\n","dataset['train'][0], dataset['test'][0], dataset['validation'][0]"],"metadata":{"collapsed":true,"id":"laeJni_Fw9pC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded = dataset.map(lambda batch: tokenizer(batch[\"text\"], padding=True, truncation=True), batched=True, batch_size=None)\n","encoded"],"metadata":{"collapsed":true,"id":"v5JtaxboypWy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Get Model and Prepare for Training"],"metadata":{"id":"j7ru3SJJ1q0Z"}},{"cell_type":"code","source":["from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(MODEL)"],"metadata":{"collapsed":true,"id":"N5uNFUJB0wSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_model_info(model, show_layers=False):\n","    \"\"\"Comprehensive model inspection\"\"\"\n","    config = model.config\n","    architecture = None\n","    model_heads = []\n","    model_type = \"Unknown\"\n","    id2label = None\n","    label2id = None\n","    merged_labels = None\n","    quant_type = \"None\"\n","    q = 0\n","\n","    gbs = model.get_memory_footprint() / 1e9\n","    param_count = model.num_parameters()\n","\n","    # Model architecture\n","    if hasattr(config, 'architectures') and config.architectures:\n","        architecture = config.architectures[0]\n","\n","    # Model heads\n","    try:\n","        if hasattr(model, 'base_model'):\n","            for module in model.modules():\n","                model_heads.append(type(module).__name__)\n","\n","                if module == model.base_model:\n","                    break\n","        else:\n","            for name, module in model.named_children()[:5]:\n","                model_heads.append(f\"{name}(type(module).__name__))\")\n","\n","        # Clean model head list\n","        model_heads = list(dict.fromkeys(model_heads))[:10]\n","    except Exception as e:\n","        model_heads = [f\"Detection failed: {str(e)[:50]}\"]\n","\n","    # Detect quantization\n","    if hasattr(config, \"quantization_config\") and config.quantization_config is not None:\n","        q_config = config.quantization_config\n","\n","        if hasattr(q_config, \"load_in_4bit\") and q_config.load_in_4bit == True:\n","            q = 4\n","            quant_type = f\"4-bit ({getattr(q_config, 'bnb_4bit_quant_type', 'unknown')})\"\n","        elif hasattr(q_config, \"load_in_8bit\") and q_config.load_in_8bit == True:\n","            q = 8\n","            quant_type = \"8-bit\"\n","    else:\n","        if hasattr(config, \"torch_dtype\") and config.torch_dtype is not None:\n","            q = config.torch_dtype.itemsize * 8\n","            quant_type = f\"FP{q} ({config.torch_dtype})\"\n","\n","    # Model type detection\n","    if hasattr(model.config, 'model_type'):\n","        model_type = model.config.model_type\n","\n","    # Label detection\n","    if  hasattr(model.config, 'label2id') and model.config.label2id is not None and hasattr(config, 'id2label') and config.id2label is not None:\n","\n","        id2label = model.config.id2label\n","        label2id = model.config.label2id\n","\n","        try:\n","            # Check for label consistency\n","            label2id_swap = {str(v): k for k, v in label2id.items()}\n","            id2label_str = {str(k): v for k, v in id2label.items()}\n","\n","            if id2label_str != label2id_swap:\n","                merged_labels = {}\n","                for k, v in id2label.items():\n","                    key = int(k) if isinstance(k, str) else k\n","                    merged_labels[key] = [v]\n","\n","                for k, v in label2id.items():\n","                    try:\n","                        v_int = int(v)\n","\n","                        if v_int in merged_labels:\n","                            merged_labels[v_int].append(k)\n","                            merged_labels[v_int] = list(set(merged_labels[v_int]))\n","                        else:\n","                            merged_labels[v_int] = [k]\n","                    except ValueError:\n","                        continue\n","\n","        except Exception as e:\n","            print(f\"Label validation error: {e}\")\n","\n","    # Basic model info\n","    print(f\"{'='*55}\")\n","    print(f\"MODEL: {getattr(config, '_name_or_path', 'Unknown')}\")\n","    print(f\"{'='*55}\")\n","\n","    print(f\"Model Type: {model_type}\")\n","\n","    if architecture is not None:\n","        print(f\"Architecture: {architecture}\")\n","\n","    if len(model_heads) > 0:\n","        print(f\"Model Structure: {' → '.join(model_heads)}\")\n","\n","    if hasattr(config, \"problem_type\") and config.problem_type is not None:\n","        print(f\"Problem Type: {config.problem_type}\")\n","\n","    if hasattr(config, \"vocab_size\"):\n","        print(f\"Vocab Size: {config.vocab_size:,}\")\n","\n","    if id2label is not None:\n","        print(\"\\nLabel Info:\")\n","\n","        if merged_labels is None:\n","            print(\"  ✅ id2label and label2id match\")\n","            print(f\"  Label count: {len(id2label)}\")\n","\n","            if len(id2label) <= 10:\n","                print(f\"  Labels: {id2label}\")\n","            else:\n","                sample_labels = dict(list(id2label.items())[:5])\n","                print(f\"  Labels (sample): {sample_labels}... (+{len(id2label)-5} more)\")\n","        else:\n","            print(\"  ⚠️ WARNING: Model id2label and label2id don't match\")\n","            print(f\"  Merged labels: {merged_labels}\")\n","\n","    print(f\"\\nParameters: {param_count:,}\")\n","    print(f\"Quantization: {quant_type}\")\n","    print(f\"Memory (actual): {gbs:.2f} GB\")\n","    print(f\"Memory (FP32 equiv): {param_count*4/1e9:.2f} GB\")\n","\n","    if gbs > 0:\n","        print(f\"Memory savings: {((param_count*4/1e9 - gbs) / (param_count*4/1e9) * 100):.1f}%\")\n","\n","    # Device info\n","    device = next(model.parameters()).device\n","    print(f\"\\nDevice: {device}\")\n","\n","    # Check if all components on same device\n","    devices = set()\n","    for name, param in model.named_parameters():\n","        devices.add(str(param.device))\n","    for name, buffer in model.named_buffers():\n","        devices.add(str(buffer.device))\n","\n","    if len(devices) > 1:\n","        print(f\"⚠️  WARNING: Model spans multiple devices: {devices}\")\n","    else:\n","        print(f\"✅ All components on: {device}\")\n","\n","    # Add training state info\n","    if hasattr(model, 'training'):\n","        mode = \"Training\" if model.training else \"Evaluation\"\n","        print(f\"\\nMode: {mode}\")\n","\n","    # Memory per layer breakdown\n","    if show_layers:\n","        print(f\"\\n{'Layer Breakdown':^55}\")\n","        print(f\"{'Layer':<30} {'Parameters':<14} {'Device'}\")\n","        print(\"-\" * 55)\n","        for name, param in model.named_parameters():\n","            # Only show layers with >1M params\n","            if param.numel() > 1000000:\n","                print(f\"{name[:28]:<30} {param.numel():>10,} {str(param.device):>10}\")"],"metadata":{"id":"QgplBj4L7Nzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_model_info(model)"],"metadata":{"id":"yXZekoEAXsEq","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label2id = {label: i for i, label in enumerate(label_names)}\n","id2label = {i: label for i, label in enumerate(label_names)}\n","\n","label2id, id2label"],"metadata":{"id":"no7eP5A419pu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForSequenceClassification, AutoConfig\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","config = AutoConfig.from_pretrained(MODEL, label2id=label2id, id2label=id2label)\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL, config=config, device_map=device)\n","\n","print(\" \" * 80)\n","show_model_info(model)"],"metadata":{"id":"zixeseeL5l35","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Model Training"],"metadata":{"id":"H1tfMb6-IF3c"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","import wandb\n","\n","batch_size = 64\n","training_dir = \"bert-base-uncased-class-trained\"\n","\n","training_args = TrainingArguments(\n","    output_dir=training_dir,\n","    overwrite_output_dir=True,\n","    num_train_epochs=2,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    weight_decay=0.01,\n","    eval_strategy=\"epoch\",\n","    disable_tqdm=False,\n","    logging_steps=10\n",")"],"metadata":{"id":"zxeLQN786SVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    acc = accuracy_score(labels, preds)\n","\n","    return {\"Accuracy\": acc, \"F1\": f1}"],"metadata":{"id":"ZpScfJplKI2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    processing_class=tokenizer,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=encoded[\"train\"],\n","    eval_dataset=encoded[\"validation\"]\n",")\n","\n","wandb.init(project=training_dir)"],"metadata":{"id":"SRA9g7WNK3u0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"QVd4FeApLT9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Log into Hugging Face\n","from google.colab import userdata\n","from huggingface_hub import login\n","login(token=userdata.get('HF_TOKEN'))"],"metadata":{"id":"eGUrmiOJPQof"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"collapsed":true,"id":"WvE6k5a6OlR0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","HF_USER = \"shayharding\"\n","classifier = pipeline(\"text-classification\", model=HF_USER + \"/\" + training_dir)"],"metadata":{"id":"HfF1xaqqQInf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier(\"Oh my God!\")"],"metadata":{"id":"mAxfDhYqQ54T"},"execution_count":null,"outputs":[]}]}