{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edce2099-f290-4c2f-beb2-00cd0a360c4a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb154c0a-68f7-4d8d-92e0-200521af33dc",
   "metadata": {},
   "source": [
    "### Load from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917a33c-fcb8-4d81-ae0c-93fbd96996c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6962f1f-eb16-491d-b611-979553cfd77c",
   "metadata": {},
   "source": [
    "> The above dataset only has a `train` dataset. Let's look at another one that has `train`, `validation`, and `test` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316934f-fe31-44b5-8910-b8f1f8742162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"knkarthick/samsum\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7920d-a41d-49f8-a80f-21f9c048fb21",
   "metadata": {},
   "source": [
    "### Preprocessing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155ac8b-b3b4-4aad-91f5-544897ba6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the original dataset\n",
    "dataset = load_dataset(\"fka/awesome-chatgpt-prompts\")\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77799060-5f96-4ced-9a55-b950637fa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].shuffle(seed=43).select(range(100))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc54407-8087-43fb-b25c-6a68f133e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "dataset = dataset.train_test_split(train_size=0.8, seed=49)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13e974-523b-4c9d-8859-6aced8d5ca2f",
   "metadata": {},
   "source": [
    "**Let's make our own dataset** from the `reuters21578/*.sgm` files. This was downloaded from https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bba36a-0e3f-4c50-9a54-9f6dbc334a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the title and body of all the articles\n",
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "dir_path = \"./reuters21578/\"\n",
    "files = os.path.join(dir_path, \"*.sgm\")\n",
    "articles = []\n",
    "\n",
    "for filepath in glob.glob(files):\n",
    "    with open(filepath, \"r\", encoding=\"latin-1\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "    for r in soup.find_all(\"reuters\"):\n",
    "        title = r.title.string if r.title else \"\"\n",
    "        body = r.body.string if r.body else \"\"\n",
    "\n",
    "        ## Clean up the results\n",
    "        if title == \"\" and body == \"\":\n",
    "            continue\n",
    "        \n",
    "        articles.append({\n",
    "            \"title\": title,\n",
    "            \"body\": body\n",
    "        })\n",
    "\n",
    "print(f\"Articles: {len(articles):,}\")\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b50d5-5367-4330-9912-bcf1f506fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's make our own dataset from these articles\n",
    "import json\n",
    "\n",
    "TRAIN_PCT = 0.8\n",
    "VALID_PCT = 0.1\n",
    "\n",
    "TRAIN_NUM = int(len(articles) * TRAIN_PCT)\n",
    "VALID_NUM = int(len(articles) * (TRAIN_PCT + VALID_PCT))\n",
    "\n",
    "# Split the data\n",
    "train_articles = articles[:TRAIN_NUM]\n",
    "print(f\"Training dataset: {len(train_articles):,}\")\n",
    "\n",
    "valid_articles = articles[TRAIN_NUM:VALID_NUM]\n",
    "print(f\"Validation dataset: {len(valid_articles):,}\")\n",
    "\n",
    "test_articles = articles[VALID_NUM:]\n",
    "print(f\"Test dataset: {len(test_articles):,}\")\n",
    "\n",
    "def save_as_jsonl(data, filename):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for article in data:\n",
    "            file.write(json.dumps(article) + \"\\n\")\n",
    "    print(f\"Wrote {filename}\")\n",
    "\n",
    "save_as_jsonl(train_articles, \"train.jsonl\")\n",
    "save_as_jsonl(valid_articles, \"valid.jsonl\")\n",
    "save_as_jsonl(test_articles, \"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4417682-34b8-4711-ab41-432fbc0e70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load them as a dataset\n",
    "data_files = {\n",
    "    \"train\": \"train.jsonl\",\n",
    "    \"validation\": \"valid.jsonl\",\n",
    "    \"test\": \"test.jsonl\"\n",
    "}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984363c-5c49-48d8-8639-40c7a1d67bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Login to Hugging Face\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800f95c-abc3-4d92-acee-2be759e825b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload dataset to Hugging Face\n",
    "dataset.push_to_hub(\"reuters-articles\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
